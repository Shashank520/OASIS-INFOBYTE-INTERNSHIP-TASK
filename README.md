# OASIS-INFOBYTE-INTERNSHIP-TASK

# Task 1- IRIS FLOWER CLASSIFICATION 

**OBJECTIVES**

The Iris dataset is a widely recognized and classic dataset in the realm of machine learning. It contains data on three distinct species of Iris flowers: Iris setosa, Iris versicolor, and Iris virginica. Frequently utilized for tasks like classification, pattern recognition, and data visualization, this dataset serves as a common benchmark in the field. The dataset contains the following attributes for each flower specimen: 

1.Sepal length in centimeters

2.Sepal width in centimeters

3.Petal length in centimeters

4.Petal width in centimeters

These characteristics act as features that play a crucial role in distinguishing among the three Iris species. The Iris dataset is frequently employed as a foundational illustration when introducing key concepts like data preprocessing, exploratory data analysis, and classification algorithms in machine learning. Based on these four features, the goal is to classify each flower into one of the three species. The dataset is balanced, meaning there is an equal number of samples for each species.

# Task 2- Sales Prediction

**OBJECTIVES**

*Data Collection: A dataset (sales_data.csv) was created with columns such as 'Date,' 'Product,' 'Price,' 'Quantity,' and 'Sales.'

*Data Preprocessing:

*Loaded the sales data into a pandas DataFrame.

*Converted the 'Date' column to datetime format.

*Extracted additional features like 'Month' and 'Day' from the 'Date' column.

*Feature Engineering:

*Selected relevant features for training the model, including 'Month,' 'Day,' 'Price,' and 'Quantity.'

*Rows & Column:: There are total 200 rows and 04 columns.

*In Data collection:: I have found various columns with help of coding which are head(),tail(),describe(),info(),row(),column(). Some of the methods used & some column to be updated and changed here. Some columns has deleted due to duplicate column present in the list & many more things happened, you can easily go through my project.

*Before visualizing any data from the Dataset we have done data Wrangling here. With the help of this we checked the null value of all the columns.After getting most null value in the columns, i dropped that columns by using "drop method".When we find minimal number of null values, filling those null values with necessary value as per as requirement by using .fillna().


# Task 3- Unemployment Analysis

**OBJECTIVES**

This analysis focuses on evaluating the impact of the COVID-19 pandemic on India's employment landscape. The dataset being examined includes vital details about unemployment rates in different Indian states. It covers essential indicators like States, Date, Measuring Frequency, Estimated Unemployment Rate (%), Estimated Employed Individuals, and Estimated Labour Participation Rate (in %).

# The objective for conducting an unemployment analysis can vary depending on the specific goals and context, but generally, the key objectives include:

*Understanding Unemployment Trends

*Identifying Causes of Unemployment

*Demographic Analysis

*Regional Analysis

*Skill Mismatch Analysis

*Forecasting Future Trends

*Comparative Analysis
